{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "\n",
    "    def init_weights(self, init_type='normal', gain=0.02):\n",
    "        '''\n",
    "        initialize network's weights\n",
    "        init_type: normal | xavier | kaiming | orthogonal\n",
    "        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n",
    "        '''\n",
    "\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "            elif classname.find('BatchNorm2d') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "\n",
    "\n",
    "class InpaintGenerator(BaseNetwork):\n",
    "    def __init__(self, in_channels=2, residual_blocks=8, out_channels=1, init_weights=True):\n",
    "        super(InpaintGenerator, self).__init__()\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True))\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),)\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(residual_blocks):\n",
    "            block = ResnetBlock(256, 2)\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.middle = nn.Sequential(*blocks)\n",
    "        \n",
    "\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True))\n",
    "        \n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True))\n",
    "        \n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=64, out_channels=out_channels, kernel_size=7, padding=0),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x, tanh=True):\n",
    "        x = self.encoder1(x)\n",
    "        print('Encoder.conv1', x.shape)\n",
    "        x = self.encoder2(x)\n",
    "        print('Encoder.conv2', x.shape)\n",
    "        x = self.encoder3(x)\n",
    "        print('Encoder.conv3', x.shape)\n",
    "        \n",
    "        x = self.middle(x)\n",
    "        print('Middle.shape', x.shape)\n",
    "        \n",
    "        \n",
    "        x = self.decoder1(x)\n",
    "        print('Decoder.conv1_tr', x.shape)\n",
    "        x = self.decoder2(x)\n",
    "        print('Decoder.conv2_tr', x.shape)\n",
    "        x = self.decoder3(x)\n",
    "        print('Decoder.conv3_tr', x.shape)\n",
    "        \n",
    "        if tanh:\n",
    "            x = torch.tanh(x)\n",
    "        else:\n",
    "            x = (torch.tanh(x) + 1) / 2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# \n",
    "class Discriminator(BaseNetwork):\n",
    "    def __init__(self, in_channels, use_sigmoid=True, use_spectral_norm=True, init_weights=True):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "        self.conv1 = self.features = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        print('conv1.shape : ', conv1.shape)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        print('conv2.shape : ', conv2.shape)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        print('conv3.shape : ', conv3.shape)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        print('conv4.shape : ', conv4.shape)\n",
    "        conv5 = self.conv5(conv4)\n",
    "        print('conv5.shape : ', conv5.shape)\n",
    "        outputs = conv5\n",
    "        if self.use_sigmoid:\n",
    "            outputs = torch.sigmoid(conv5)\n",
    "            print('out.shape : ', outputs.shape)\n",
    "\n",
    "        return outputs, [conv1, conv2, conv3, conv4, conv5]\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dilation=1, use_spectral_norm=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(dilation),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "#         print('intermediate.shape : ', out.shape)\n",
    "        # Remove ReLU at the end of the residual block\n",
    "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def spectral_norm(module, mode=True):\n",
    "    if mode:\n",
    "        return nn.utils.spectral_norm(module)\n",
    "\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,9,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = InpaintGenerator(in_channels=9, out_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder.conv1 torch.Size([1, 64, 64, 64])\n",
      "Encoder.conv2 torch.Size([1, 128, 32, 32])\n",
      "Encoder.conv3 torch.Size([1, 256, 16, 16])\n",
      "Middle.shape torch.Size([1, 256, 16, 16])\n",
      "Decoder.conv1_tr torch.Size([1, 128, 32, 32])\n",
      "Decoder.conv2_tr torch.Size([1, 64, 64, 64])\n",
      "Decoder.conv3_tr torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1,9,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.shape :  torch.Size([1, 64, 32, 32])\n",
      "conv2.shape :  torch.Size([1, 128, 16, 16])\n",
      "conv3.shape :  torch.Size([1, 256, 8, 8])\n",
      "conv4.shape :  torch.Size([1, 512, 7, 7])\n",
      "conv5.shape :  torch.Size([1, 1, 6, 6])\n",
      "out.shape :  torch.Size([1, 1, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "           [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "           [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "           [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "           [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "           [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]]]],\n",
       "        grad_fn=<SigmoidBackward>), [tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<LeakyReluBackward1>),\n",
       "  tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<LeakyReluBackward1>),\n",
       "  tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<LeakyReluBackward1>),\n",
       "  tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<LeakyReluBackward1>),\n",
       "  tensor([[[[0., 0., 0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0., 0., 0.],\n",
       "            [0., 0., 0., 0., 0., 0.]]]], grad_fn=<MkldnnConvolutionBackward>)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
